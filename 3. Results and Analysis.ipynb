{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eaf3991e",
   "metadata": {},
   "source": [
    "# Here, we do analysis using the validation/test results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5fd6de2",
   "metadata": {},
   "outputs": [],
   "source": [
    "top = 1   # if we choose multiple top results of each method, However, we have (many) 60 datasets, so top 1 is enough."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b18947fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "from scipy.stats import kurtosis, skew\n",
    "from itertools import combinations\n",
    "import minepy\n",
    "from collections import Counter\n",
    "from sklearn.metrics import mutual_info_score\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state = 2)   # 5-fold-cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a190b95e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import bayesiantests as bt\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#https://matplotlib.org/stable/gallery/lines_bars_and_markers/horizontal_barchart_distribution.html\n",
    "def stacked_bar(results, category_names):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    results : dict\n",
    "        A mapping from question labels to a list of answers per category.\n",
    "        It is assumed all lists contain the same number of entries and that\n",
    "        it matches the length of *category_names*.\n",
    "    category_names : list of str\n",
    "        The category labels.\n",
    "    \"\"\"\n",
    "    labels = list(results.keys())\n",
    "    data = np.array(list(results.values()))\n",
    "    data_cum = data.cumsum(axis=1)\n",
    "    category_colors = plt.colormaps['RdYlGn'](\n",
    "        np.linspace(0.15, 0.85, data.shape[1]))\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(9.2, 5))\n",
    "    ax.invert_yaxis()\n",
    "    ax.xaxis.set_visible(False)\n",
    "    ax.set_xlim(0, np.sum(data, axis=1).max())\n",
    "\n",
    "    for i, (colname, color) in enumerate(zip(category_names, category_colors)):\n",
    "        widths = data[:, i]\n",
    "        starts = data_cum[:, i] - widths\n",
    "        rects = ax.barh(labels, widths, left=starts, height=0.5,\n",
    "                        label=colname, color=color)\n",
    "\n",
    "        r, g, b, _ = color\n",
    "        text_color = 'white' if r * g * b < 0.5 else 'darkgrey'\n",
    "        ax.bar_label(rects, label_type='center', color=text_color)\n",
    "    ax.legend(ncols=len(category_names), bbox_to_anchor=(0, 1),\n",
    "              loc='upper left', fontsize='small')\n",
    "\n",
    "    return fig, ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "275a4f4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def BST(rope, baselines, ours, dfs):\n",
    "    comp = []\n",
    "    basewin = []\n",
    "    draw = []\n",
    "    ourswin = []\n",
    "    z = 0\n",
    "    for i in range(len(ours)):\n",
    "        for j in range(len(baselines)):\n",
    "            names = (baselines[j],ours[i])\n",
    "            comp.append(names)\n",
    "            X = np.array(dfs[i][[baselines[j],ours[i]]])\n",
    "            left, within, right = bt.signtest(X, rope=rope, verbose=True, names=names)\n",
    "            basewin.append(left)\n",
    "            draw.append(within)\n",
    "            ourswin.append(right)        \n",
    "    results = pd.DataFrame(comp, columns = [\"Baseline\",\"Ours\"])\n",
    "    results[\"Basewin_prob\"] = basewin\n",
    "    results[\"Draw_prob\"] = draw\n",
    "    results[\"Ourswin_prob\"] = ourswin\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec8910d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_list = [18, 41, 14, 43, 53, 28, 20, 63, 69, 56, 19, 25, 6, 24, 80, 32, 22, 15, 27, 33, 58,\n",
    "              46, 29, 64, 62, 17, 47, 13, 44, 9, 49, 55, 3, 35, 67, 54, 12, 7, 39, 36, 4, 79, 59, 52, 5, 57,\n",
    "              21, 50, 45, 42, 11, 1, 51, 38, 34, 16, 10, 2, 26, 91]\n",
    "print(len(final_list), final_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca945c8e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_meta_loaded = pd.read_csv(\"df_meta_new.csv\", index_col='Unnamed: 0')\n",
    "df_meta_loaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1496457",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "new_index = []\n",
    "for i in range(len(df_meta_loaded)):\n",
    "    if list(df_meta_loaded['data'])[i] in final_list:\n",
    "        new_index.append(i)\n",
    "df_meta = df_meta_loaded.iloc[new_index,:]\n",
    "df_meta.reset_index(inplace=True, drop=True)\n",
    "df_meta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5c11c30",
   "metadata": {},
   "source": [
    "# Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2909a77",
   "metadata": {},
   "source": [
    "###  find the best-performing for each method in each results of (SMOTE&LLM) and (LSH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78c6967d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "df = pd.read_csv('result.csv', low_memory=False)\n",
    "df.rename(columns={'Unnamed: 0':'Metrics'}, inplace=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dcfd9c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_LS = pd.read_csv('result_LS.csv', low_memory=False)\n",
    "df_LS.rename(columns={'Unnamed: 0':'Metrics'}, inplace=True)\n",
    "df_LS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "340e2c63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# F1\n",
    "base_ind = 2\n",
    "num_class = (df.shape[1]-base_ind)/(1+5+5)\n",
    "org_ind = int(base_ind+num_class)\n",
    "sm_ind = int(org_ind+(df.shape[1]-base_ind-num_class)/2)\n",
    "metrics = ['Acc', 'Pre', 'Rec', 'Spe', 'F1', 'GM', 'BA', 'AUC'] # 8:Acc, 9:Pre, 10:Rec, 11:Spe, 12:F1, 13:GM, 14:BA, 15:AUC \n",
    "df_res = pd.DataFrame({'DATA':[0 for i in range(top*len(final_list))],\n",
    "                       'ORG_tr':[0 for i in range(top*len(final_list))],\n",
    "                       'ORG_va':[0 for i in range(top*len(final_list))],\n",
    "                       'ORG_t':[0 for i in range(top*len(final_list))],\n",
    "                       'SMOTE_tr':[0 for i in range(top*len(final_list))],\n",
    "                       'SMOTE_va':[0 for i in range(top*len(final_list))],\n",
    "                       'SMOTE_t':[0 for i in range(top*len(final_list))],\n",
    "                       'LLM_tr':[0 for i in range(top*len(final_list))],\n",
    "                       'LLM_va':[0 for i in range(top*len(final_list))],\n",
    "                       'LLM_t':[0 for i in range(top*len(final_list))]})\n",
    "\n",
    "for i in range(len(final_list)):        # in one dataset\n",
    "    for j in range(len(metrics)*2):       # in one metric\n",
    "        if j == 12:                      # only choose one metric (12:validation_F1)\n",
    "            df_i = df[df[\"Dataset\"] == str(final_list[i])]   # i_th dataset\n",
    "            df_i_base = df_i.iloc[:,:base_ind]               # i_th dataset base\n",
    "            df_i_org = df_i.iloc[:,base_ind:org_ind]         # i_th dataset original results (12 classifiers)\n",
    "            df_i_sm = df_i.iloc[:,org_ind:sm_ind]            # i_th dataset smote results (12 classifier X 5 Resam strategy)\n",
    "            df_i_lm = df_i.iloc[:,sm_ind:]                   # i_th dataset llm results (12 classifier X 5 Resam strategy)  \n",
    "            best_org = pd.DataFrame(df_i_org.iloc[j,:]).iloc[:,0].sort_values(ascending=False)[:top].index\n",
    "            df_i_org_best = df_i_org.loc[:,best_org] \n",
    "            best_sm = pd.DataFrame(df_i_sm.iloc[j,:]).iloc[:,0].sort_values(ascending=False)[:top].index\n",
    "            df_i_sm_best = df_i_sm.loc[:,best_sm]\n",
    "            best_lm = pd.DataFrame(df_i_lm.iloc[j,:]).iloc[:,0].sort_values(ascending=False)[:top].index\n",
    "            df_i_lm_best = df_i_lm.loc[:,best_lm]\n",
    "            df_i_best = pd.concat([df_i_base,df_i_org_best],axis=1)\n",
    "            df_i_best = pd.concat([df_i_best,df_i_sm_best],axis=1)\n",
    "            df_i_best = pd.concat([df_i_best,df_i_lm_best],axis=1)\n",
    "            for k in range(top):\n",
    "                df_res.iloc[i*top+k:i*top+k+1,0] = final_list[i]\n",
    "                df_res.iloc[i*top+k:i*top+k+1,1] = list(df_i_best.iloc[[j-8],2+k])\n",
    "                df_res.iloc[i*top+k:i*top+k+1,2] = list(df_i_best.iloc[[j],2+k])\n",
    "                df_res.iloc[i*top+k:i*top+k+1,3] = list(df_i_best.iloc[[j+8],2+k])\n",
    "                df_res.iloc[i*top+k:i*top+k+1,4] = list(df_i_best.iloc[[j-8],2+top+k])\n",
    "                df_res.iloc[i*top+k:i*top+k+1,5] = list(df_i_best.iloc[[j],2+top+k])\n",
    "                df_res.iloc[i*top+k:i*top+k+1,6] = list(df_i_best.iloc[[j+8],2+top+k])\n",
    "                df_res.iloc[i*top+k:i*top+k+1,7] = list(df_i_best.iloc[[j-8],2+top+top+k])\n",
    "                df_res.iloc[i*top+k:i*top+k+1,8] = list(df_i_best.iloc[[j],2+top+top+k])\n",
    "                df_res.iloc[i*top+k:i*top+k+1,9] = list(df_i_best.iloc[[j+8],2+top+top+k])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8392dc5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# F1-LS\n",
    "df_res_LS = pd.DataFrame({'LS_tr':[0 for i in range(top*len(final_list))],\n",
    "                          'LS_va':[0 for i in range(top*len(final_list))],\n",
    "                          'LS_t':[0 for i in range(top*len(final_list))]})\n",
    "for i in range(len(final_list)):        # in one dataset\n",
    "    for j in range(len(metrics)*2):       # in one metric\n",
    "        if j == 12:                      # only choose one metric (12:validation_F1)\n",
    "            df_i = df_LS[df_LS[\"Dataset\"] == str(final_list[i])]   # i_th dataset\n",
    "            df_i_base = df_i.iloc[:,:base_ind]               # i_th dataset base\n",
    "            df_i_LS = df_i.iloc[:,base_ind:]                 # i_th dataset LS results \n",
    "            best_LS = pd.DataFrame(df_i_LS.iloc[j,:]).iloc[:,0].sort_values(ascending=False)[:top].index\n",
    "            df_i_LS_best = df_i_LS.loc[:,best_LS]     \n",
    "            df_i_best = pd.concat([df_i_base,df_i_LS_best],axis=1)\n",
    "            for k in range(top):\n",
    "                df_res_LS.iloc[i*top+k:i*top+k+1,0] = list(df_i_best.iloc[[j-8],2+k])\n",
    "                df_res_LS.iloc[i*top+k:i*top+k+1,1] = list(df_i_best.iloc[[j],2+k])\n",
    "                df_res_LS.iloc[i*top+k:i*top+k+1,2] = list(df_i_best.iloc[[j+8],2+k])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d928e482",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_res_final = pd.concat([df_res, df_res_LS], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02c0b4e4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_best_scores = df_res_final\n",
    "df_best_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3e8ad5f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Averaged Results  (when top>1 is used, this code calculates average scores)\n",
    "df_best_scores_avg = pd.DataFrame(final_list, columns=['DATA'])\n",
    "for i in range(1, df_best_scores.shape[1]):\n",
    "    avg_list = []\n",
    "    for j in range(len(df_best_scores_avg)):\n",
    "        avg_list.append(np.average(df_best_scores.iloc[j*top:j*top+top,i]))\n",
    "    df_best_scores_avg[f'{df_best_scores.columns[i]}'] = avg_list\n",
    "df_best_scores_avg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75d58d8e",
   "metadata": {},
   "source": [
    "## 1.Performance - comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36b785e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Table 1 (SMOTE/BSM/ADA/GPT/LSH-G)\n",
    "# Performance Margin\n",
    "df_best_scores_avg['(LS-SM)_t'] = df_best_scores_avg.loc[:,'LS_t'].astype(float) - df_best_scores_avg.loc[:,'SMOTE_t'].astype(float)\n",
    "df_best_scores_avg['(LS-BS)_t'] = df_best_scores_avg.loc[:,'LS_t'].astype(float) - df_best_scores_avg.loc[:,'BSM_t'].astype(float)\n",
    "df_best_scores_avg['(LS-AD)_t'] = df_best_scores_avg.loc[:,'LS_t'].astype(float) - df_best_scores_avg.loc[:,'ADA_t'].astype(float)\n",
    "df_best_scores_avg['(LS-LM)_t'] = df_best_scores_avg.loc[:,'LS_t'].astype(float) - df_best_scores_avg.loc[:,'LLM_t'].astype(float)\n",
    "\n",
    "# Winning Number\n",
    "df_best_scores_avg['(LS>SM)_t'] = [1 if df_best_scores_avg.loc[i,\"LS_t\"] > df_best_scores_avg.loc[i,\"SMOTE_t\"] else 0 for i in range(len(df_best_scores_avg))]\n",
    "df_best_scores_avg['(LS=SM)_t'] = [1 if df_best_scores_avg.loc[i,\"LS_t\"] == df_best_scores_avg.loc[i,\"SMOTE_t\"] else 0 for i in range(len(df_best_scores_avg))]\n",
    "df_best_scores_avg['(LS<SM)_t'] = [1 if df_best_scores_avg.loc[i,\"LS_t\"] < df_best_scores_avg.loc[i,\"SMOTE_t\"] else 0 for i in range(len(df_best_scores_avg))]\n",
    "df_best_scores_avg['(LS>BS)_t'] = [1 if df_best_scores_avg.loc[i,\"LS_t\"] > df_best_scores_avg.loc[i,\"BSM_t\"] else 0 for i in range(len(df_best_scores_avg))]\n",
    "df_best_scores_avg['(LS=BS)_t'] = [1 if df_best_scores_avg.loc[i,\"LS_t\"] == df_best_scores_avg.loc[i,\"BSM_t\"] else 0 for i in range(len(df_best_scores_avg))]\n",
    "df_best_scores_avg['(LS<BS)_t'] = [1 if df_best_scores_avg.loc[i,\"LS_t\"] < df_best_scores_avg.loc[i,\"BSM_t\"] else 0 for i in range(len(df_best_scores_avg))]\n",
    "df_best_scores_avg['(LS>AD)_t'] = [1 if df_best_scores_avg.loc[i,\"LS_t\"] > df_best_scores_avg.loc[i,\"ADA_t\"] else 0 for i in range(len(df_best_scores_avg))]\n",
    "df_best_scores_avg['(LS=AD)_t'] = [1 if df_best_scores_avg.loc[i,\"LS_t\"] == df_best_scores_avg.loc[i,\"ADA_t\"] else 0 for i in range(len(df_best_scores_avg))]\n",
    "df_best_scores_avg['(LS<AD)_t'] = [1 if df_best_scores_avg.loc[i,\"LS_t\"] < df_best_scores_avg.loc[i,\"ADA_t\"] else 0 for i in range(len(df_best_scores_avg))]\n",
    "df_best_scores_avg['(LS>LM)_t'] = [1 if df_best_scores_avg.loc[i,\"LS_t\"] > df_best_scores_avg.loc[i,\"LLM_t\"] else 0 for i in range(len(df_best_scores_avg))]\n",
    "df_best_scores_avg['(LS=LM)_t'] = [1 if df_best_scores_avg.loc[i,\"LS_t\"] == df_best_scores_avg.loc[i,\"LLM_t\"] else 0 for i in range(len(df_best_scores_avg))]\n",
    "df_best_scores_avg['(LS<LM)_t'] = [1 if df_best_scores_avg.loc[i,\"LS_t\"] < df_best_scores_avg.loc[i,\"LLM_t\"] else 0 for i in range(len(df_best_scores_avg))]\n",
    "\n",
    "df_best_scores_avg.iloc[:,16:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5184d188",
   "metadata": {},
   "outputs": [],
   "source": [
    "# winning numbers\n",
    "print(sum(df_best_scores_avg['(LS>SM)_t']), sum(df_best_scores_avg['(LS=SM)_t']), sum(df_best_scores_avg['(LS<SM)_t']))\n",
    "print(sum(df_best_scores_avg['(LS>BS)_t']), sum(df_best_scores_avg['(LS=BS)_t']), sum(df_best_scores_avg['(LS<BS)_t']))\n",
    "print(sum(df_best_scores_avg['(LS>AD)_t']), sum(df_best_scores_avg['(LS=AD)_t']), sum(df_best_scores_avg['(LS<AD)_t']))\n",
    "print(sum(df_best_scores_avg['(LS>LM)_t']), sum(df_best_scores_avg['(LS=LM)_t']), sum(df_best_scores_avg['(LS<LM)_t']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49ccc5dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# average performance\n",
    "print(\"SM_t\", np.average(df_best_scores_avg.loc[:,'SMOTE_t']))\n",
    "print(\"BS_t\", np.average(df_best_scores_avg.loc[:,'BSM_t']))\n",
    "print(\"AD_t\", np.average(df_best_scores_avg.loc[:,'ADA_t']))\n",
    "print(\"LM_t\", np.average(df_best_scores_avg.loc[:,'LLM_t']))\n",
    "print(\"LS_t\", np.average(df_best_scores_avg.loc[:,'LS_t']))\n",
    "\n",
    "# performance margin\n",
    "print(\"avg. margin (LS-SM)_t:\", np.average(df_best_scores_avg.loc[:,'(LS-SM)_t']))\n",
    "print(\"avg. margin (LS-BS)_t:\", np.average(df_best_scores_avg.loc[:,'(LS-BS)_t']))\n",
    "print(\"avg. margin (LS-AD)_t:\", np.average(df_best_scores_avg.loc[:,'(LS-AD)_t']))\n",
    "print(\"avg. margin (LS-LM)_t:\", np.average(df_best_scores_avg.loc[:,'(LS-LM)_t']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a0a0c18",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4f7b0b08",
   "metadata": {},
   "source": [
    "## 2. Performance by imbalance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d4f2cbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMBALANCE GROUPS\n",
    "print(len(df_best_scores_avg.iloc[:15,:]))\n",
    "print(len(df_best_scores_avg.iloc[15:30:,:]))\n",
    "print(len(df_best_scores_avg.iloc[30:44,:]))\n",
    "print(len(df_best_scores_avg.iloc[44:,:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4af80d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Relative Improvement (# Table 1 (SMOTE/BSM/ADA/GPT/LSH-G))\n",
    "df_rel = df_best_scores_avg.loc[:,[\"(LS-SM)_t\", \"SMOTE_t\", \"(LS-BS)_t\", \"BSM_t\", \"(LS-AD)_t\", \"ADA_t\", \"(LS-LM)_t\", \"LLM_t\"]]\n",
    "df_rel[\"SM_rel\"] = df_rel[\"(LS-SM)_t\"] / df_rel[\"SMOTE_t\"] * 100\n",
    "df_rel[\"BS_rel\"] = df_rel[\"(LS-BS)_t\"] / df_rel[\"BSM_t\"] * 100\n",
    "df_rel[\"AD_rel\"] = df_rel[\"(LS-AD)_t\"] / df_rel[\"ADA_t\"] * 100\n",
    "df_rel[\"LM_rel\"] = df_rel[\"(LS-LM)_t\"] / df_rel[\"LLM_t\"] * 100\n",
    "df_rel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d14dc99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_rel = df_rel.replace(np.inf, np.nan)\n",
    "df_rel = df_rel.replace(np.inf, 100)  \n",
    "\n",
    "print(\"LS-SM\", \"less\", np.mean(df_rel.iloc[44:,8]))\n",
    "print(\"LS-BS\", \"less\", np.mean(df_rel.iloc[44:,9]))\n",
    "print(\"LS-AD\", \"less\", np.mean(df_rel.iloc[44:,10]))\n",
    "print(\"LS-LM\", \"less\", np.mean(df_rel.iloc[44:,11]))\n",
    "print('*'*20)\n",
    "print(\"LS-SM\", \"mod\", np.mean(df_rel.iloc[30:44,8]))\n",
    "print(\"LS-BS\", \"mod\", np.mean(df_rel.iloc[30:44,9]))\n",
    "print(\"LS-AD\", \"mod\", np.mean(df_rel.iloc[30:44,10]))\n",
    "print(\"LS-LM\", \"mod\", np.mean(df_rel.iloc[30:44,11]))\n",
    "print('*'*20)\n",
    "print(\"LS-SM\", \"mid\", np.mean(df_rel.iloc[15:30,8]))\n",
    "print(\"LS-BS\", \"mid\", np.mean(df_rel.iloc[15:30,9]))\n",
    "print(\"LS-AD\", \"mid\", np.mean(df_rel.iloc[15:30,10]))\n",
    "print(\"LS-LM\", \"mid\", np.mean(df_rel.iloc[15:30,11]))\n",
    "print('*'*20)\n",
    "print(\"LS-SM\", \"less\", np.mean(df_rel.iloc[:15,8]))\n",
    "print(\"LS-BS\", \"less\", np.mean(df_rel.iloc[:15,9]))\n",
    "print(\"LS-AD\", \"less\", np.mean(df_rel.iloc[:15,10]))\n",
    "print(\"LS-LM\", \"less\", np.mean(df_rel.iloc[:15,11]))\n",
    "print('*'*20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6e0709e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2aa4022a",
   "metadata": {},
   "source": [
    "## 3. Robustness - Achievement Rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57f2eec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Achievement Rate (AR_t/va) - Table 1\n",
    "df_best_scores_avg['SM_t/SM_va'] = df_best_scores_avg.loc[:,'SMOTE_t'].astype(float) / df_best_scores_avg.loc[:,'SMOTE_va'].astype(float)\n",
    "df_best_scores_avg['LM_t/LM_va'] = df_best_scores_avg.loc[:,'LLM_t'].astype(float) / df_best_scores_avg.loc[:,'LLM_va'].astype(float)\n",
    "df_best_scores_avg['LS_t/LS_va'] = df_best_scores_avg.loc[:,'LS_t'].astype(float) / df_best_scores_avg.loc[:,'LS_va'].astype(float)\n",
    "\n",
    "# Achievement Rate (AR_t/va) - Table 2\n",
    "df_best_scores_avg_2['BSM_t/BSM_va'] = df_best_scores_avg_2.loc[:,'BSM_t'].astype(float) / df_best_scores_avg_2.loc[:,'BSM_va'].astype(float)\n",
    "df_best_scores_avg_2['ADA_t/ADA_va'] = df_best_scores_avg_2.loc[:,'ADA_t'].astype(float) / df_best_scores_avg_2.loc[:,'ADA_va'].astype(float)\n",
    "df_best_scores_avg_2['LM2_t/LM2_va'] = df_best_scores_avg_2.loc[:,'DEV_t'].astype(float) / df_best_scores_avg_2.loc[:,'DEV_va'].astype(float)\n",
    "df_best_scores_avg_2['LS2_t/LS2_va'] = df_best_scores_avg_2.loc[:,'LS2_t'].astype(float) / df_best_scores_avg_2.loc[:,'LS2_va'].astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cf59f26",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_AR = pd.concat([df_best_scores_avg.iloc[:,-3:], df_best_scores_avg_2.iloc[:,-4:]], axis=1)\n",
    "df_AR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64812659",
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing/validation\n",
    "print(\"SM\", np.average(df_AR.loc[:,\"SM_t/SM_va\"]), np.std(df_AR.loc[:,\"SM_t/SM_va\"]))\n",
    "print(\"BSM\", np.average(df_AR.loc[:,\"BSM_t/BSM_va\"]), np.std(df_AR.loc[:,\"BSM_t/BSM_va\"]))\n",
    "print(\"ADA\", np.average(df_AR.loc[:,\"ADA_t/ADA_va\"]), np.std(df_AR.loc[:,\"ADA_t/ADA_va\"]))\n",
    "print(\"LM\", np.average(df_AR.loc[:,\"LM_t/LM_va\"]), np.std(df_AR.loc[:,\"LM_t/LM_va\"]))\n",
    "print(\"LS\", np.average(df_AR.loc[:,\"LS_t/LS_va\"]), np.std(df_AR.loc[:,\"LS_t/LS_va\"]))\n",
    "print(\"LM2\", np.average(df_AR.loc[:,\"LM2_t/LM2_va\"]), np.std(df_AR.loc[:,\"LM2_t/LM2_va\"]))\n",
    "print(\"LS2\", np.average(df_AR.loc[:,\"LS2_t/LS2_va\"]), np.std(df_AR.loc[:,\"LS2_t/LS2_va\"]))\n",
    "\n",
    "print(\"SM\", np.var(df_AR.loc[:,\"SM_t/SM_va\"]))\n",
    "print(\"BSM\", np.var(df_AR.loc[:,\"BSM_t/BSM_va\"]))\n",
    "print(\"ADA\", np.var(df_AR.loc[:,\"ADA_t/ADA_va\"]))\n",
    "print(\"LM\", np.var(df_AR.loc[:,\"LM_t/LM_va\"]))\n",
    "print(\"LS\", np.var(df_AR.loc[:,\"LS_t/LS_va\"]))\n",
    "print(\"LM2\", np.var(df_AR.loc[:,\"LM2_t/LM2_va\"]))\n",
    "print(\"LS2\", np.var(df_AR.loc[:,\"LS2_t/LS2_va\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78b5ed11",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
