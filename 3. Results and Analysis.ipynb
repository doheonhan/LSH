{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5fd6de2",
   "metadata": {},
   "outputs": [],
   "source": [
    "top = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b18947fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "from scipy.stats import kurtosis, skew\n",
    "from itertools import combinations\n",
    "import minepy\n",
    "from collections import Counter\n",
    "from sklearn.metrics import mutual_info_score\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state = 2)   # 5-fold-cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a190b95e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import bayesiantests as bt\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#https://matplotlib.org/stable/gallery/lines_bars_and_markers/horizontal_barchart_distribution.html\n",
    "def stacked_bar(results, category_names):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    results : dict\n",
    "        A mapping from question labels to a list of answers per category.\n",
    "        It is assumed all lists contain the same number of entries and that\n",
    "        it matches the length of *category_names*.\n",
    "    category_names : list of str\n",
    "        The category labels.\n",
    "    \"\"\"\n",
    "    labels = list(results.keys())\n",
    "    data = np.array(list(results.values()))\n",
    "    data_cum = data.cumsum(axis=1)\n",
    "    category_colors = plt.colormaps['RdYlGn'](\n",
    "        np.linspace(0.15, 0.85, data.shape[1]))\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(9.2, 5))\n",
    "    ax.invert_yaxis()\n",
    "    ax.xaxis.set_visible(False)\n",
    "    ax.set_xlim(0, np.sum(data, axis=1).max())\n",
    "\n",
    "    for i, (colname, color) in enumerate(zip(category_names, category_colors)):\n",
    "        widths = data[:, i]\n",
    "        starts = data_cum[:, i] - widths\n",
    "        rects = ax.barh(labels, widths, left=starts, height=0.5,\n",
    "                        label=colname, color=color)\n",
    "\n",
    "        r, g, b, _ = color\n",
    "        text_color = 'white' if r * g * b < 0.5 else 'darkgrey'\n",
    "        ax.bar_label(rects, label_type='center', color=text_color)\n",
    "    ax.legend(ncols=len(category_names), bbox_to_anchor=(0, 1),\n",
    "              loc='upper left', fontsize='small')\n",
    "\n",
    "    return fig, ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "275a4f4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def BST(rope, baselines, ours, dfs):\n",
    "    comp = []\n",
    "    basewin = []\n",
    "    draw = []\n",
    "    ourswin = []\n",
    "    z = 0\n",
    "    for i in range(len(ours)):\n",
    "        for j in range(len(baselines)):\n",
    "            names = (baselines[j],ours[i])\n",
    "            comp.append(names)\n",
    "            X = np.array(dfs[i][[baselines[j],ours[i]]])\n",
    "            left, within, right = bt.signtest(X, rope=rope, verbose=True, names=names)\n",
    "            basewin.append(left)\n",
    "            draw.append(within)\n",
    "            ourswin.append(right)        \n",
    "    results = pd.DataFrame(comp, columns = [\"Baseline\",\"Ours\"])\n",
    "    results[\"Basewin_prob\"] = basewin\n",
    "    results[\"Draw_prob\"] = draw\n",
    "    results[\"Ourswin_prob\"] = ourswin\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec8910d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_list = [18, 41, 14, 43, 53, 28, 20, 63, 69, 56, 19, 25, 6, 24, 80, 32, 22, 15, 27, 33, 58,\n",
    "              46, 29, 64, 62, 17, 47, 13, 44, 9, 49, 55, 3, 35, 67, 54, 12, 7, 39, 36, 4, 79, 59, 52, 5, 57,\n",
    "              21, 50, 45, 42, 11, 1, 51, 38, 34, 16, 10, 2, 26, 91]\n",
    "print(len(final_list), final_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca945c8e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_meta_loaded = pd.read_csv(\"df_meta_new.csv\", index_col='Unnamed: 0')\n",
    "df_meta_loaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1496457",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "new_index = []\n",
    "for i in range(len(df_meta_loaded)):\n",
    "    if list(df_meta_loaded['data'])[i] in final_list:\n",
    "        new_index.append(i)\n",
    "df_meta = df_meta_loaded.iloc[new_index,:]\n",
    "df_meta.reset_index(inplace=True, drop=True)\n",
    "df_meta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5c11c30",
   "metadata": {},
   "source": [
    "# Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78c6967d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "df = pd.read_csv('result.csv', low_memory=False)\n",
    "df.rename(columns={'Unnamed: 0':'Metrics'}, inplace=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dcfd9c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_LS = pd.read_csv('result_LS.csv', low_memory=False)\n",
    "df_LS.rename(columns={'Unnamed: 0':'Metrics'}, inplace=True)\n",
    "df_LS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "340e2c63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# F1\n",
    "base_ind = 2\n",
    "num_class = (df.shape[1]-base_ind)/(1+5+5)\n",
    "org_ind = int(base_ind+num_class)\n",
    "sm_ind = int(org_ind+(df.shape[1]-base_ind-num_class)/2)\n",
    "metrics = ['Acc', 'Pre', 'Rec', 'Spe', 'F1', 'GM', 'BA', 'AUC'] # 8:Acc, 9:Pre, 10:Rec, 11:Spe, 12:F1, 13:GM, 14:BA, 15:AUC \n",
    "df_res = pd.DataFrame({'DATA':[0 for i in range(top*len(final_list))],\n",
    "                       'ORG_tr':[0 for i in range(top*len(final_list))],\n",
    "                       'ORG_va':[0 for i in range(top*len(final_list))],\n",
    "                       'ORG_t':[0 for i in range(top*len(final_list))],\n",
    "                       'SMOTE_tr':[0 for i in range(top*len(final_list))],\n",
    "                       'SMOTE_va':[0 for i in range(top*len(final_list))],\n",
    "                       'SMOTE_t':[0 for i in range(top*len(final_list))],\n",
    "                       'LLM_tr':[0 for i in range(top*len(final_list))],\n",
    "                       'LLM_va':[0 for i in range(top*len(final_list))],\n",
    "                       'LLM_t':[0 for i in range(top*len(final_list))]})\n",
    "\n",
    "for i in range(len(final_list)):        # in one dataset\n",
    "    for j in range(len(metrics)*2):       # in one metric\n",
    "        if j == 12:                      # only choose one metric (12:validation_F1)\n",
    "            df_i = df[df[\"Dataset\"] == str(final_list[i])]   # i_th dataset\n",
    "            df_i_base = df_i.iloc[:,:base_ind]               # i_th dataset base\n",
    "            df_i_org = df_i.iloc[:,base_ind:org_ind]         # i_th dataset original results (12 classifiers)\n",
    "            df_i_sm = df_i.iloc[:,org_ind:sm_ind]            # i_th dataset smote results (12 classifier X 5 Resam strategy)\n",
    "            df_i_lm = df_i.iloc[:,sm_ind:]                   # i_th dataset llm results (12 classifier X 5 Resam strategy)  \n",
    "            best_org = pd.DataFrame(df_i_org.iloc[j,:]).iloc[:,0].sort_values(ascending=False)[:top].index\n",
    "            df_i_org_best = df_i_org.loc[:,best_org] \n",
    "            best_sm = pd.DataFrame(df_i_sm.iloc[j,:]).iloc[:,0].sort_values(ascending=False)[:top].index\n",
    "            df_i_sm_best = df_i_sm.loc[:,best_sm]\n",
    "            best_lm = pd.DataFrame(df_i_lm.iloc[j,:]).iloc[:,0].sort_values(ascending=False)[:top].index\n",
    "            df_i_lm_best = df_i_lm.loc[:,best_lm]\n",
    "            df_i_best = pd.concat([df_i_base,df_i_org_best],axis=1)\n",
    "            df_i_best = pd.concat([df_i_best,df_i_sm_best],axis=1)\n",
    "            df_i_best = pd.concat([df_i_best,df_i_lm_best],axis=1)\n",
    "            for k in range(top):\n",
    "                df_res.iloc[i*top+k:i*top+k+1,0] = final_list[i]\n",
    "                df_res.iloc[i*top+k:i*top+k+1,1] = list(df_i_best.iloc[[j-8],2+k])\n",
    "                df_res.iloc[i*top+k:i*top+k+1,2] = list(df_i_best.iloc[[j],2+k])\n",
    "                df_res.iloc[i*top+k:i*top+k+1,3] = list(df_i_best.iloc[[j+8],2+k])\n",
    "                df_res.iloc[i*top+k:i*top+k+1,4] = list(df_i_best.iloc[[j-8],2+top+k])\n",
    "                df_res.iloc[i*top+k:i*top+k+1,5] = list(df_i_best.iloc[[j],2+top+k])\n",
    "                df_res.iloc[i*top+k:i*top+k+1,6] = list(df_i_best.iloc[[j+8],2+top+k])\n",
    "                df_res.iloc[i*top+k:i*top+k+1,7] = list(df_i_best.iloc[[j-8],2+top+top+k])\n",
    "                df_res.iloc[i*top+k:i*top+k+1,8] = list(df_i_best.iloc[[j],2+top+top+k])\n",
    "                df_res.iloc[i*top+k:i*top+k+1,9] = list(df_i_best.iloc[[j+8],2+top+top+k])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8392dc5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# F1-LS\n",
    "df_res_LS = pd.DataFrame({'LS_tr':[0 for i in range(top*len(final_list))],\n",
    "                          'LS_va':[0 for i in range(top*len(final_list))],\n",
    "                          'LS_t':[0 for i in range(top*len(final_list))]})\n",
    "for i in range(len(final_list)):        # in one dataset\n",
    "    for j in range(len(metrics)*2):       # in one metric\n",
    "        if j == 12:                      # only choose one metric (12:validation_F1)\n",
    "            df_i = df_LS[df_LS[\"Dataset\"] == str(final_list[i])]   # i_th dataset\n",
    "            df_i_base = df_i.iloc[:,:base_ind]               # i_th dataset base\n",
    "            df_i_LS = df_i.iloc[:,base_ind:]                 # i_th dataset LS results \n",
    "            best_LS = pd.DataFrame(df_i_LS.iloc[j,:]).iloc[:,0].sort_values(ascending=False)[:top].index\n",
    "            df_i_LS_best = df_i_LS.loc[:,best_LS]     \n",
    "            df_i_best = pd.concat([df_i_base,df_i_LS_best],axis=1)\n",
    "            for k in range(top):\n",
    "                df_res_LS.iloc[i*top+k:i*top+k+1,0] = list(df_i_best.iloc[[j-8],2+k])\n",
    "                df_res_LS.iloc[i*top+k:i*top+k+1,1] = list(df_i_best.iloc[[j],2+k])\n",
    "                df_res_LS.iloc[i*top+k:i*top+k+1,2] = list(df_i_best.iloc[[j+8],2+k])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d928e482",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_res_final = pd.concat([df_res, df_res_LS], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02c0b4e4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_best_scores = df_res_final\n",
    "df_best_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3e8ad5f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Averaged Results\n",
    "df_best_scores_avg = pd.DataFrame(final_list, columns=['DATA'])\n",
    "for i in range(1, df_best_scores.shape[1]):\n",
    "    avg_list = []\n",
    "    for j in range(len(df_best_scores_avg)):\n",
    "        avg_list.append(np.average(df_best_scores.iloc[j*top:j*top+top,i]))\n",
    "    df_best_scores_avg[f'{df_best_scores.columns[i]}'] = avg_list\n",
    "df_best_scores_avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88b8e4a8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Performance Margin (val)\n",
    "df_best_scores_avg['(LM-SM)_va'] = df_best_scores_avg.loc[:,'LLM_va'].astype(float) - df_best_scores_avg.loc[:,'SMOTE_va'].astype(float)\n",
    "df_best_scores_avg['(LS-LM)_va'] = df_best_scores_avg.loc[:,'LS_va'].astype(float) - df_best_scores_avg.loc[:,'LLM_va'].astype(float)\n",
    "df_best_scores_avg['(LS-SM)_va'] = df_best_scores_avg.loc[:,'LS_va'].astype(float) - df_best_scores_avg.loc[:,'SMOTE_va'].astype(float)\n",
    "# Performance Margin (test)\n",
    "df_best_scores_avg['(LM-SM)_t'] = df_best_scores_avg.loc[:,'LLM_t'].astype(float) - df_best_scores_avg.loc[:,'SMOTE_t'].astype(float)\n",
    "df_best_scores_avg['(LS-LM)_t'] = df_best_scores_avg.loc[:,'LS_t'].astype(float) - df_best_scores_avg.loc[:,'LLM_t'].astype(float)\n",
    "df_best_scores_avg['(LS-SM)_t'] = df_best_scores_avg.loc[:,'LS_t'].astype(float) - df_best_scores_avg.loc[:,'SMOTE_t'].astype(float)\n",
    "\n",
    "# Decision (val)\n",
    "df_best_scores_avg['(LM>SM)_va'] = [1 if df_best_scores_avg.loc[i,\"LLM_va\"] > df_best_scores_avg.loc[i,\"SMOTE_va\"] else 0 for i in range(len(df_best_scores_avg))]\n",
    "df_best_scores_avg['(LM=SM)_va'] = [1 if df_best_scores_avg.loc[i,\"LLM_va\"] == df_best_scores_avg.loc[i,\"SMOTE_va\"] else 0 for i in range(len(df_best_scores_avg))]\n",
    "df_best_scores_avg['(LS>LM)_va'] = [1 if df_best_scores_avg.loc[i,\"LS_va\"] > df_best_scores_avg.loc[i,\"LLM_va\"] else 0 for i in range(len(df_best_scores_avg))]\n",
    "df_best_scores_avg['(LS=LM)_va'] = [1 if df_best_scores_avg.loc[i,\"LS_va\"] == df_best_scores_avg.loc[i,\"LLM_va\"] else 0 for i in range(len(df_best_scores_avg))]\n",
    "df_best_scores_avg['(LS>SM)_va'] = [1 if df_best_scores_avg.loc[i,\"LS_va\"] > df_best_scores_avg.loc[i,\"SMOTE_va\"] else 0 for i in range(len(df_best_scores_avg))]\n",
    "df_best_scores_avg['(LS=SM)_va'] = [1 if df_best_scores_avg.loc[i,\"LS_va\"] == df_best_scores_avg.loc[i,\"SMOTE_va\"] else 0 for i in range(len(df_best_scores_avg))]\n",
    "# Decision (test)\n",
    "df_best_scores_avg['(LM>SM)_t'] = [1 if df_best_scores_avg.loc[i,\"LLM_t\"] > df_best_scores_avg.loc[i,\"SMOTE_t\"] else 0 for i in range(len(df_best_scores_avg))]\n",
    "df_best_scores_avg['(LM=SM)_t'] = [1 if df_best_scores_avg.loc[i,\"LLM_t\"] == df_best_scores_avg.loc[i,\"SMOTE_t\"] else 0 for i in range(len(df_best_scores_avg))]\n",
    "df_best_scores_avg['(LS>LM)_t'] = [1 if df_best_scores_avg.loc[i,\"LS_t\"] > df_best_scores_avg.loc[i,\"LLM_t\"] else 0 for i in range(len(df_best_scores_avg))]\n",
    "df_best_scores_avg['(LS=LM)_t'] = [1 if df_best_scores_avg.loc[i,\"LS_t\"] == df_best_scores_avg.loc[i,\"LLM_t\"] else 0 for i in range(len(df_best_scores_avg))]\n",
    "df_best_scores_avg['(LS>SM)_t'] = [1 if df_best_scores_avg.loc[i,\"LS_t\"] > df_best_scores_avg.loc[i,\"SMOTE_t\"] else 0 for i in range(len(df_best_scores_avg))]\n",
    "df_best_scores_avg['(LS=SM)_t'] = [1 if df_best_scores_avg.loc[i,\"LS_t\"] == df_best_scores_avg.loc[i,\"SMOTE_t\"] else 0 for i in range(len(df_best_scores_avg))]\n",
    "\n",
    "# Achievement Rate (AR_va/tr)\n",
    "df_best_scores_avg['SM_va/SM_tr'] = df_best_scores_avg.loc[:,'SMOTE_va'].astype(float) / df_best_scores_avg.loc[:,'SMOTE_tr'].astype(float)\n",
    "df_best_scores_avg['LM_va/LM_tr'] = df_best_scores_avg.loc[:,'LLM_va'].astype(float) / df_best_scores_avg.loc[:,'LLM_tr'].astype(float)\n",
    "df_best_scores_avg['LS_va/LS_tr'] = df_best_scores_avg.loc[:,'LS_va'].astype(float) / df_best_scores_avg.loc[:,'LS_tr'].astype(float)\n",
    "# Achievement Rate (AR_t/tr)\n",
    "df_best_scores_avg['SM_t/SM_tr'] = df_best_scores_avg.loc[:,'SMOTE_t'].astype(float) / df_best_scores_avg.loc[:,'SMOTE_tr'].astype(float)\n",
    "df_best_scores_avg['LM_t/LM_tr'] = df_best_scores_avg.loc[:,'LLM_t'].astype(float) / df_best_scores_avg.loc[:,'LLM_tr'].astype(float)\n",
    "df_best_scores_avg['LS_t/LS_tr'] = df_best_scores_avg.loc[:,'LS_t'].astype(float) / df_best_scores_avg.loc[:,'LS_tr'].astype(float)\n",
    "# Achievement Rate (AR_t/va)\n",
    "df_best_scores_avg['SM_t/SM_va'] = df_best_scores_avg.loc[:,'SMOTE_t'].astype(float) / df_best_scores_avg.loc[:,'SMOTE_va'].astype(float)\n",
    "df_best_scores_avg['LM_t/LM_va'] = df_best_scores_avg.loc[:,'LLM_t'].astype(float) / df_best_scores_avg.loc[:,'LLM_va'].astype(float)\n",
    "df_best_scores_avg['LS_t/LS_va'] = df_best_scores_avg.loc[:,'LS_t'].astype(float) / df_best_scores_avg.loc[:,'LS_va'].astype(float)\n",
    "\n",
    "df_best_scores_avg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75d58d8e",
   "metadata": {},
   "source": [
    "## 1.1 Performance - SMOTE vs LLM vs LS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68fdf89c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# winning numbers (test)\n",
    "print(sum(df_best_scores_avg['(LM>SM)_t']), sum(df_best_scores_avg['(LS>LM)_t']), sum(df_best_scores_avg['(LS>SM)_t']))\n",
    "print(sum(df_best_scores_avg['(LM=SM)_t']), sum(df_best_scores_avg['(LS=LM)_t']), sum(df_best_scores_avg['(LS=SM)_t']))\n",
    "print(len(df_best_scores_avg)-sum(df_best_scores_avg['(LM>SM)_t'])-sum(df_best_scores_avg['(LM=SM)_t']),\n",
    "      len(df_best_scores_avg)-sum(df_best_scores_avg['(LS>LM)_t'])-sum(df_best_scores_avg['(LS=LM)_t']),\n",
    "      len(df_best_scores_avg)-sum(df_best_scores_avg['(LS>SM)_t'])-sum(df_best_scores_avg['(LS=SM)_t']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4675b049",
   "metadata": {},
   "source": [
    "## 1.2 Performance Margin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49d3d9c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test\n",
    "print(\"ORG_t\", np.average(df_best_scores_avg.loc[:,'ORG_t']))\n",
    "print(\"SM_t\", np.average(df_best_scores_avg.loc[:,'SMOTE_t']))\n",
    "print(\"LM_t\", np.average(df_best_scores_avg.loc[:,'LLM_t']))\n",
    "print(\"LS_t\", np.average(df_best_scores_avg.loc[:,'LS_t']))\n",
    "print(\"avg. margin (LM-SM)_t:\", np.average(df_best_scores_avg.loc[:,'(LM-SM)_t']))\n",
    "print(\"avg. margin (LS-LM)_t:\", np.average(df_best_scores_avg.loc[:,'(LS-LM)_t']))\n",
    "print(\"avg. margin (LS-SM)_t:\", np.average(df_best_scores_avg.loc[:,'(LS-SM)_t']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe2914f8",
   "metadata": {},
   "source": [
    "## 1-3. Winning Probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "703dd400",
   "metadata": {},
   "outputs": [],
   "source": [
    "rope = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a103ddc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b233222e",
   "metadata": {},
   "outputs": [],
   "source": [
    "BST(rope, ['SMOTE_t'], ['LLM_t'], [df_best_scores_avg])  # BST(rope, baselines, ours, dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89077097",
   "metadata": {},
   "outputs": [],
   "source": [
    "BST(rope, ['LLM_t'], ['LS_t'], [df_best_scores_avg])  # BST(rope, baselines, ours, dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "487f03f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "BST(rope, ['SMOTE_t'], ['LS_t'], [df_best_scores_avg])  # BST(rope, baselines, ours, dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a0a0c18",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4f7b0b08",
   "metadata": {},
   "source": [
    "## 2.1 Performance in Imbalance - SMOTE vs LLM vs LS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d4f2cbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "half = int(len(final_list)/3)\n",
    "half"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63abd3b2",
   "metadata": {},
   "source": [
    "## 2.2 Performance Margin in Imbalance (LLM-SMOTE) | (LLM-LS) | (LS-SMOTE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28c95e33",
   "metadata": {},
   "outputs": [],
   "source": [
    "## test\n",
    "# in more imbalanced\n",
    "print(\"avg. margin (LM-SM)_t:\", np.average(df_best_scores_avg.loc[:half-1,'(LM-SM)_t']))\n",
    "# in mid imbalanced\n",
    "print(\"avg. margin (LM-SM)_t:\", np.average(df_best_scores_avg.loc[half:2*half-1,'(LM-SM)_t']))\n",
    "# in less imbalanced\n",
    "print(\"avg. margin (LM-SM)_t:\", np.average(df_best_scores_avg.loc[2*half:,'(LM-SM)_t']))\n",
    "\n",
    "# in more imbalanced\n",
    "print(\"avg. margin (LS-LM)_t:\", np.average(df_best_scores_avg.loc[:half-1,'(LS-LM)_t']))\n",
    "# in mid imbalanced\n",
    "print(\"avg. margin (LS-LM)_t:\", np.average(df_best_scores_avg.loc[half:2*half-1,'(LS-LM)_t']))\n",
    "# in less imbalanced\n",
    "print(\"avg. margin (LS-LM)_t:\", np.average(df_best_scores_avg.loc[2*half:,'(LS-LM)_t']))\n",
    "\n",
    "# in more imbalanced\n",
    "print(\"avg. margin (LS-SM)_t:\", np.average(df_best_scores_avg.loc[:half-1,'(LS-SM)_t']))\n",
    "# in mid imbalanced\n",
    "print(\"avg. margin (LS-SM)_t:\", np.average(df_best_scores_avg.loc[half:2*half-1,'(LS-SM)_t']))\n",
    "# in less imbalanced\n",
    "print(\"avg. margin (LS-SM)_t:\", np.average(df_best_scores_avg.loc[2*half:,'(LS-SM)_t']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aff37cb0",
   "metadata": {},
   "source": [
    "## 2.3 Winning Probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60b94e98",
   "metadata": {},
   "outputs": [],
   "source": [
    "rope = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efdadd14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# in more imbalanced\n",
    "BST(rope, ['SMOTE_t'], ['LLM_t'], [df_best_scores_avg.iloc[:half,:]])  # BST(rope, baselines, ours, dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2830ab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mid imbalanced\n",
    "BST(rope, ['SMOTE_t'], ['LLM_t'], [df_best_scores_avg.iloc[half:-half,:]])  # BST(rope, baselines, ours, dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c89aa18d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# in less imbalanced\n",
    "BST(rope, ['SMOTE_t'], ['LLM_t'], [df_best_scores_avg.iloc[-half:,:]])  # BST(rope, baselines, ours, dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a473b6b1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8652e473",
   "metadata": {},
   "outputs": [],
   "source": [
    "# in more imbalanced\n",
    "BST(rope, ['LLM_t'], ['LS_t'], [df_best_scores_avg.iloc[:half,:]])  # BST(rope, baselines, ours, dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67772e35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mid imbalanced\n",
    "BST(rope, ['LLM_t'], ['LS_t'], [df_best_scores_avg.iloc[half:-half,:]])  # BST(rope, baselines, ours, dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "148770b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# in less imbalanced\n",
    "BST(rope, ['LLM_t'], ['LS_t'], [df_best_scores_avg.iloc[-half:,:]])  # BST(rope, baselines, ours, dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94fbeebc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00d14e84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# in more imbalanced\n",
    "BST(rope, ['SMOTE_t'], ['LS_t'], [df_best_scores_avg.iloc[:half,:]])  # BST(rope, baselines, ours, dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2694b57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mid imbalanced\n",
    "BST(rope, ['SMOTE_t'], ['LS_t'], [df_best_scores_avg.iloc[half:-half,:]])  # BST(rope, baselines, ours, dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3e9a31b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# in less imbalanced\n",
    "BST(rope, ['SMOTE_t'], ['LS_t'], [df_best_scores_avg.iloc[-half:,:]])  # BST(rope, baselines, ours, dfs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15478d01",
   "metadata": {},
   "source": [
    "## 2.4 Correlation Analysis with Margin (LLM-SMOTE) | (LLM-LS) | (LS-SMOTE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78ce0609",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.rcParams[\"figure.figsize\"] = [5, 3]\n",
    "plt.rcParams[\"figure.dpi\"] = 200\n",
    "plt.rcParams['axes.titlesize'] = 20  \n",
    "plt.rcParams['axes.linewidth'] = 2\n",
    "plt.rcParams['axes.labelsize'] = 20  \n",
    "plt.rcParams['font.size'] = 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84e229df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization\n",
    "for i in range(3,4):\n",
    "    X = df_meta.iloc[:,i]   # imbalance ratio\n",
    "    Y = df_best_scores_avg.loc[:,'(LM-SM)_t']  # 'Margin' Or 'Decision'\n",
    "    plt.scatter(X, Y, alpha=0.5)\n",
    "    plt.title('Correlation of (LM-SM) & IR')\n",
    "    plt.xlabel('IR (X:1)')\n",
    "    plt.ylabel('(LM-SM)')\n",
    "    plt.show()\n",
    "    \n",
    "# chatGPT 추천\n",
    "import pandas as pd\n",
    "from scipy.stats import pearsonr, spearmanr\n",
    "\n",
    "# Convert to pandas Series\n",
    "imb = pd.Series(X)\n",
    "ach = pd.Series(Y)\n",
    "\n",
    "# Pearson correlation (linear)\n",
    "pearson_corr, pearson_p = pearsonr(imb, ach)\n",
    "\n",
    "# Spearman correlation (rank-based)\n",
    "spearman_corr, spearman_p = spearmanr(imb, ach)\n",
    "\n",
    "print(\"Pearson correlation:\", pearson_corr, \" (p =\", pearson_p, \")\")\n",
    "print(\"Spearman correlation:\", spearman_corr, \" (p =\", spearman_p, \")\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "589df521",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization\n",
    "for i in range(3,4):\n",
    "    X = df_meta.iloc[:,i]   # imbalance ratio\n",
    "    Y = df_best_scores_avg.loc[:,'(LS-LM)_t']  # 'Margin' Or 'Decision'\n",
    "    plt.scatter(X, Y, alpha=0.5)\n",
    "    plt.title('Correlation of (LS-LM) & IR')\n",
    "    plt.xlabel('IR (X:1)')\n",
    "    plt.ylabel('(LS-LM)')\n",
    "    plt.show()\n",
    "    \n",
    "# chatGPT 추천\n",
    "import pandas as pd\n",
    "from scipy.stats import pearsonr, spearmanr\n",
    "\n",
    "# Convert to pandas Series\n",
    "imb = pd.Series(X)\n",
    "ach = pd.Series(Y)\n",
    "\n",
    "# Pearson correlation (linear)\n",
    "pearson_corr, pearson_p = pearsonr(imb, ach)\n",
    "\n",
    "# Spearman correlation (rank-based)\n",
    "spearman_corr, spearman_p = spearmanr(imb, ach)\n",
    "\n",
    "print(\"Pearson correlation:\", pearson_corr, \" (p =\", pearson_p, \")\")\n",
    "print(\"Spearman correlation:\", spearman_corr, \" (p =\", spearman_p, \")\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "876ae29b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization\n",
    "for i in range(3,4):\n",
    "    X = df_meta.iloc[:,i]   # imbalance ratio\n",
    "    Y = df_best_scores_avg.loc[:,'(LS-SM)_t']  # 'Margin' Or 'Decision'\n",
    "    plt.scatter(X, Y, alpha=0.5)\n",
    "    plt.title('Correlation of (LS-SM) & IR')\n",
    "    plt.xlabel('IR (X:1)')\n",
    "    plt.ylabel('(LS-SM)')\n",
    "    plt.show()\n",
    "    \n",
    "# chatGPT 추천\n",
    "import pandas as pd\n",
    "from scipy.stats import pearsonr, spearmanr\n",
    "\n",
    "# Convert to pandas Series\n",
    "imb = pd.Series(X)\n",
    "ach = pd.Series(Y)\n",
    "\n",
    "# Pearson correlation (linear)\n",
    "pearson_corr, pearson_p = pearsonr(imb, ach)\n",
    "\n",
    "# Spearman correlation (rank-based)\n",
    "spearman_corr, spearman_p = spearmanr(imb, ach)\n",
    "\n",
    "print(\"Pearson correlation:\", pearson_corr, \" (p =\", pearson_p, \")\")\n",
    "print(\"Spearman correlation:\", spearman_corr, \" (p =\", spearman_p, \")\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6e0709e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2aa4022a",
   "metadata": {},
   "source": [
    "## 3.1 Overfitting - Achievement Rate of SMOTE vs LLM vs LS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb250a69",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_best_scores_avg.loc[:,['DATA',\n",
    "                          'SM_va/SM_tr','LM_va/LM_tr','LS_va/LS_tr',\n",
    "                          'SM_t/SM_tr','LM_t/LM_tr','LS_t/LS_tr',\n",
    "                          'SM_t/SM_va','LM_t/LM_va','LS_t/LS_va']]   \n",
    "# if <1 overfitting, smaller->more overfitting\n",
    "# especially, if it significantly >1 or <1, overfitting and underfitting\n",
    "# 각 데이터별로 비교가 좋을 듯"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b019f36",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# testing/validation\n",
    "print(\"SM\", np.average(df_best_scores_avg.loc[:,\"SM_t/SM_va\"]), np.std(df_best_scores_avg.loc[:,\"SM_t/SM_va\"]))\n",
    "print(\"LM\", np.average(df_best_scores_avg.loc[:,\"LM_t/LM_va\"]), np.std(df_best_scores_avg.loc[:,\"LM_t/LM_va\"]))\n",
    "print(\"LS\", np.average(df_best_scores_avg.loc[:,\"LS_t/LS_va\"]), np.std(df_best_scores_avg.loc[:,\"LS_t/LS_va\"]))\n",
    "\n",
    "print(\"SM\", np.var(df_best_scores_avg.loc[:,\"SM_t/SM_va\"]))\n",
    "print(\"LM\", np.var(df_best_scores_avg.loc[:,\"LM_t/LM_va\"]))\n",
    "print(\"LS\", np.var(df_best_scores_avg.loc[:,\"LS_t/LS_va\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "248e5427",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1abd8e78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization\n",
    "for i in range(3,4):\n",
    "    X = df_meta.iloc[:,i]   # imbalance ratio\n",
    "    Y = df_best_scores_avg.loc[:,\"SM_t/SM_va\"]  # Achievment Rate\n",
    "    plt.scatter(X, Y, alpha=0.5)\n",
    "    plt.title('Correlation of (Test/Val)_SM & IR')\n",
    "    plt.xlabel('IR (X:1)')\n",
    "    plt.ylabel('(Test/Val)_SM')\n",
    "    plt.show()\n",
    "    \n",
    "import pandas as pd\n",
    "from scipy.stats import pearsonr, spearmanr\n",
    "\n",
    "# Convert to pandas Series\n",
    "imb = pd.Series(X)\n",
    "ach = pd.Series(Y)\n",
    "\n",
    "# Pearson correlation (linear)\n",
    "pearson_corr, pearson_p = pearsonr(imb, ach)\n",
    "\n",
    "# Spearman correlation (rank-based)\n",
    "spearman_corr, spearman_p = spearmanr(imb, ach)\n",
    "\n",
    "print(\"Pearson correlation:\", pearson_corr, \" (p =\", pearson_p, \")\")\n",
    "print(\"Spearman correlation:\", spearman_corr, \" (p =\", spearman_p, \")\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "692ab38d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization\n",
    "for i in range(3,4):\n",
    "    X = df_meta.iloc[:,i]   # imbalance ratio\n",
    "    Y = df_best_scores_avg.loc[:,\"LM_t/LM_va\"]  # Achievment Rate\n",
    "    plt.scatter(X, Y, alpha=0.5)\n",
    "    plt.title('Correlation of (Test/Val)_LM & IR')\n",
    "    plt.xlabel('IR (X:1)')\n",
    "    plt.ylabel('(Test/Val)_LM')\n",
    "    plt.show()\n",
    "    \n",
    "import pandas as pd\n",
    "from scipy.stats import pearsonr, spearmanr\n",
    "\n",
    "# Convert to pandas Series\n",
    "imb = pd.Series(X)\n",
    "ach = pd.Series(Y)\n",
    "\n",
    "# Pearson correlation (linear)\n",
    "pearson_corr, pearson_p = pearsonr(imb, ach)\n",
    "\n",
    "# Spearman correlation (rank-based)\n",
    "spearman_corr, spearman_p = spearmanr(imb, ach)\n",
    "\n",
    "print(\"Pearson correlation:\", pearson_corr, \" (p =\", pearson_p, \")\")\n",
    "print(\"Spearman correlation:\", spearman_corr, \" (p =\", spearman_p, \")\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b5ce598",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization\n",
    "for i in range(3,4):\n",
    "    X = df_meta.iloc[:,i]   # imbalance ratio\n",
    "    Y = df_best_scores_avg.loc[:,\"LS_t/LS_va\"]  # Achievment Rate\n",
    "    plt.scatter(X, Y, alpha=0.5)\n",
    "    plt.title('Correlation of (Test/Val)_LS & IR')\n",
    "    plt.xlabel('IR (X:1)')\n",
    "    plt.ylabel('(Test/Val)_LS')\n",
    "    plt.ylim(-0.05, 1.25)\n",
    "    plt.show()\n",
    "    \n",
    "import pandas as pd\n",
    "from scipy.stats import pearsonr, spearmanr\n",
    "\n",
    "# Convert to pandas Series\n",
    "imb = pd.Series(X)\n",
    "ach = pd.Series(Y)\n",
    "\n",
    "# Pearson correlation (linear)\n",
    "pearson_corr, pearson_p = pearsonr(imb, ach)\n",
    "\n",
    "# Spearman correlation (rank-based)\n",
    "spearman_corr, spearman_p = spearmanr(imb, ach)\n",
    "\n",
    "print(\"Pearson correlation:\", pearson_corr, \" (p =\", pearson_p, \")\")\n",
    "print(\"Spearman correlation:\", spearman_corr, \" (p =\", spearman_p, \")\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78b5ed11",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
